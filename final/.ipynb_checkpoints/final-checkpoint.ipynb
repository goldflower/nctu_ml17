{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part I: preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 1: get the faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2, os, sys\n",
    "import scipy.misc as misc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "sys.path.insert(0, 'FaceDetect-master')\n",
    "import multiprocessing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "age = ['adult', 'child', 'elder', 'young']\n",
    "sexual = ['male', 'female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_face_helper(file, face_cascade, min_neighbor=3):\n",
    "    count = 0\n",
    "    try_rescale = False\n",
    "    img = cv2.imread(file)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=min_neighbor)\n",
    "    if scale is None and len(faces) == 0:\n",
    "        return gray\n",
    "    elif scale and len(faces) == 0: # hass fail for origin image\n",
    "        try_rescale = True\n",
    "        faces, img, gray = get_scaling_faces(gray, scale)\n",
    "        if len(faces) == 0: # haas fail for rescaled image\n",
    "            return gray              \n",
    "        continue\n",
    "        \n",
    "    has_face = False\n",
    "    processed_img = None\n",
    "    for (row, col, height, width), i in zip(faces, range(len(faces))):\n",
    "        if height < min(img.shape[0], img.shape[1])*0.2:\n",
    "            if i == len(faces)-1: # hass fail\n",
    "                return gray                       \n",
    "            else:                     \n",
    "                continue\n",
    "        elif processed_img is None:               \n",
    "            processed_img = img[col:col+width, row:row+height]\n",
    "        elif height > processed_img.shape[0]: # keep the largest square              \n",
    "            processed_img = img[col:col+width, row:row+height]\n",
    "    else:\n",
    "        # success for retriving face\n",
    "        return processed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-148-36469ab13cd3>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-148-36469ab13cd3>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    if second_detector:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def get_faces(data_path='data/', save_data_path='processed', age = ['adult', 'child', 'elder', 'young'], \n",
    "              sexual = ['male', 'female'], preserve_all=True, min_neighbor=3, second_detector=False,\n",
    "              scale = None, write_origin=True, get_face_from_eyes=False, testing=False):\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "    counts = [0, 0, 0]\n",
    "    if testing:\n",
    "        for file in os.listdir(data_path):\n",
    "            \n",
    "    if second_detector:\n",
    "        import FaceFinder\n",
    "        import tensorflow as tf\n",
    "        model_path = 'FaceDetect-master/face_model'\n",
    "        def tf_detector_helper(file_path):\n",
    "            tf.reset_default_graph()\n",
    "            img = cv2.imread(path + file, 0)\n",
    "            tf_faces, mask = FaceFinder.localize(img, model_path)\n",
    "            return tf_faces, mask\n",
    "    if scale:\n",
    "        def get_scaling_faces(img, scale):\n",
    "            i = cv2.resize(img, (int(img.shape[0]*scale), img.shape[1]))\n",
    "            g = cv2.resize(gray, (int(gray.shape[0]*scale), gray.shape[1]))\n",
    "            faces = face_cascade.detectMultiScale(g, scaleFactor=1.3, minNeighbors=min_neighbor)\n",
    "            \n",
    "            if len(faces) == 0:\n",
    "                i = cv2.resize(img, (img.shape[0], int(img.shape[1]*scale)))\n",
    "                g = cv2.resize(gray, (gray.shape[0], int(gray.shape[1]*scale)))\n",
    "                faces = face_cascade.detectMultiScale(g, scaleFactor=1.3, minNeighbors=min_neighbor)\n",
    "                \n",
    "            return faces, i, g\n",
    "    resolved_by_scale = pd.DataFrame(columns = ['age', 'sexual', 'file', 'success'])\n",
    "    \n",
    "    for a in age:\n",
    "        for s in sexual:\n",
    "            print(a, s, min_neighbor)\n",
    "            path = data_path + a + '/' + s + '/'\n",
    "            save_path = save_data_path + str(min_neighbor) + '/' + a + '/' + s + '/'\n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "            files = os.listdir(path)\n",
    "            chunk = len(files) // 80\n",
    "            for file, count in zip(files, range(len(files))):\n",
    "                try_rescale = False\n",
    "                if count % chunk == 0:\n",
    "                    sys.stdout.write(\"\\r[%s%s]\" % ('=' * (count//chunk), ' ' * (80-count//chunk)))\n",
    "                img = cv2.imread(path + file)\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                if get_face_from_eyes:\n",
    "                    img_ = img.copy()\n",
    "                    gray_ = gray.copy()\n",
    "                faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=min_neighbor)\n",
    "                if write_origin:\n",
    "                    cv2.imwrite((save_path + file).replace('.','_o.'), img)\n",
    "                if scale is None and len(faces) == 0:\n",
    "                    if second_detector: # use tensorflow\n",
    "                        tf_faces, mask = tf_detector_helper(path+file)\n",
    "                        if tf_faces is None: # tf fail\n",
    "                            if preserve_all:\n",
    "                                cv2.imwrite(save_path + file, img)\n",
    "                        else:\n",
    "                            cv2.imwrite(save_path + file, tf_faces)\n",
    "                    elif not second_detector and perserve_all:\n",
    "                        csv.imwrite(save_path + file, img)\n",
    "                    continue\n",
    "                elif scale and len(faces) == 0: # hass fail for origin image\n",
    "                    try_rescale = True\n",
    "                    faces, img, gray = get_scaling_faces(gray, scale)\n",
    "                    if second_detector: # use tensorflow\n",
    "                        tf_faces, mask = tf_detector_helper(path+file)\n",
    "                        if tf_faces is None: # tf fail\n",
    "                            if preserve_all:\n",
    "#                                 cv2.imwrite(save_path + file, img)\n",
    "                                print('')\n",
    "                        else:\n",
    "                            cv2.imwrite(save_path + file, tf_faces)\n",
    "                    if len(faces) == 0: # haas fail for rescaled image\n",
    "                        if get_face_from_eyes:\n",
    "                            face = get_face_from_eyes_helper(img_)\n",
    "                            if len(face) == 0:\n",
    "                                cv2.imwrite(save_path + file.replace('.', '_o.'), img)\n",
    "                                count[0] += 1\n",
    "                            else:\n",
    "                                cv2.imwrite(save_path + file.replace('.', '_e.'), face)                        \n",
    "                        #resolved_by_scale = resolved_by_scale.append({'age':a, 'sexual':s, 'file':file, 'success':False}, ignore_index=True)\n",
    "                        elif preserve_all and not write_origin:\n",
    "                            cv2.imwrite(save_path + file.format('.','_o.'), img)\n",
    "                            count[0] += 1\n",
    "                        continue\n",
    "                    \n",
    "                \n",
    "                has_face = False\n",
    "                processed_img = None\n",
    "                for (row, col, height, width), i in zip(faces, range(len(faces))):\n",
    "                    if height < min(img.shape[0], img.shape[1])*0.2:\n",
    "                        if i == len(faces)-1: # hass fail\n",
    "                            if second_detector: # use tensorflow\n",
    "                                tf_faces, mask = tf_detector_helper(path+file)\n",
    "                                if preserve_all and not write_origin and tf_faces is None:\n",
    "                                    cv2.imwrite(save_path + file, img)\n",
    "                                else:\n",
    "                                    cv2.imwrite(save_path + file, tf_faces)\n",
    "                            elif get_face_from_eyes:\n",
    "                                face = get_face_from_eyes_helper(img_)\n",
    "                                if len(face) == 0:\n",
    "                                    cv2.imwrite(save_path + file.replace('.', '_o.'), img)\n",
    "                                else:\n",
    "                                    cv2.imwrite(save_path + file.replace('.', '_e.'), face)\n",
    "                            elif preserve_all and not write_origin:\n",
    "                                if try_rescale:\n",
    "                                    resolved_by_scale = resolved_by_scale.append({'age':a, 'sexual':s, 'file':file, 'success':False}, ignore_index=True)\n",
    "                                cv2.imwrite(save_path + file.replace('.', '_e.'), img)                            \n",
    "                            break\n",
    "                        else:                     \n",
    "                            continue\n",
    "                    elif processed_img is None:               \n",
    "                        processed_img = img[col:col+width, row:row+height]\n",
    "                    elif height > processed_img.shape[0]: # keep the largest square              \n",
    "                        processed_img = img[col:col+width, row:row+height]\n",
    "                else:\n",
    "                    # success for retriving face\n",
    "                    if try_rescale:\n",
    "                        resolved_by_scale = resolved_by_scale.append({'age':a, 'sexual':s, 'file':file, 'success':True}, ignore_index=True)\n",
    "                    cv2.imwrite(save_path + file.replace('.', '_f.'), processed_img)\n",
    "                    count[1] += 1\n",
    "            print()\n",
    "    if write_origin:\n",
    "        return resolved_by_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[463, 247,  44,  44],\n",
       "       [904, 542,  50,  50]], dtype=int32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACwAAAAsCAIAAACR5s1WAAANnElEQVR4nE1Y2ZYbx5GNLTOrCmig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\nRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=44x44 at 0x1172776A0>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('data/adult/male/10.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "eyes = eye_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=10)\n",
    "has_eye = False\n",
    "processed_imgs = []\n",
    "\n",
    "left_eye = eyes[np.argmin(eyes, axis=0)[0]]\n",
    "right_eye = eyes[np.argmax(eyes, axis=0)[0]]\n",
    "eye_square_width = left_eye[2]\n",
    "face = [max(left_eye[0]-eye_square_width*3, 0), max(left_eye[1]-eye_square_width*4, 0), eye_square_width*7, eye_square_width*6]\n",
    "face = img[face[0]:min(face[0]+face[2], img.shape[1]), face[1]:min(face[1]+face[3], img.shape[0])]\n",
    "#misc.toimage(face)\n",
    "misc.toimage(img[eyes[0][0]:eyes[0][0]+eyes[0][2], eyes[0][1]:eyes[0][1]+eyes[0][3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_face_from_eyes_helper(img, min_neighbor=3):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    eyes = eye_cascade.detectMultiScale(gray, scaleFactor=1.5, minNeighbors=min_neighbor)\n",
    "    has_eye = False\n",
    "    processed_imgs = []\n",
    "\n",
    "    if len(eyes) == 2:\n",
    "        left_eye = eyes[np.argmin(eyes, axis=0)[0]]\n",
    "        right_eye = eyes[np.argmax(eyes, axis=0)[0]]\n",
    "        eye_square_width = left_eye[2] \n",
    "        if left_eye[0] + 2*eye_square_width < right_eye[0]:\n",
    "            return []\n",
    "        face = [max(left_eye[0]-eye_square_width*3, 0), max(left_eye[1]-eye_square_width*4, 0), eye_square_width*7, eye_square_width*6]\n",
    "        face = img[face[0]:min(face[0]+face[2], img.shape[1]), face[1]:min(face[1]+face[3], img.shape[0])]\n",
    "        return face\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult male 3\n",
      "[===================================================================================]\n",
      "adult female 3\n",
      "[================================================================================]\n",
      "child male 3\n",
      "[=================================================================================]\n",
      "child female 3\n",
      "[=====================================================================================]\n",
      "elder male 3\n",
      "[=======================================================================================]\n",
      "elder female 3\n",
      "[==========================================================================================]\n",
      "young male 3\n",
      "[==================================================================================]\n",
      "young female 3\n",
      "[===================================================================================]\n"
     ]
    }
   ],
   "source": [
    "get_faces(save_data_path='preserve_and_resolve', scale=1.2, write_origin=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-8b1d3102bac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'success'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "result[result['success']==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 2: resize the faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def change_sizes(data_path = 'processed3', target_size=None, data_save_path=None, \n",
    "                 plot=False, resize_method='median', mode=None):\n",
    "    \"\"\" resize given images\n",
    "    resize_method: \n",
    "        resize the image size according to the given method\n",
    "        \"median\", \"max\", \"min\", \"mean\"\n",
    "    mode:\n",
    "        mode to convert the image\n",
    "        'RGB'(default), 'L'(grey scale)\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    methods = {'median':np.median, 'mean':np.mean, 'max':np.max, 'min':np.min}\n",
    "    if target_size is None:\n",
    "        sizes = []\n",
    "        for a in age:\n",
    "            for s in sexual:\n",
    "                print(a, s)\n",
    "                path = data_path + '/{}/{}/'.format(a, s)\n",
    "                files = os.listdir(path)\n",
    "                for file in files:\n",
    "                    img = misc.imread(path + file)\n",
    "                    sizes.append(img.shape[0])\n",
    "        target_size = methods[resize_method](sizes)\n",
    "        if plot:\n",
    "            n, bins, patches = plt.hist(sizes, bins=100, normed=1, facecolor='green', alpha=0.75)\n",
    "    #         mu, sigma = np.mean(sizes), np.std(sizes)\n",
    "    #         y = mlab.normpdf(bins, mu, sigma)\n",
    "    #         l = plt.plot(bins, y, 'r--', linewidth=1)\n",
    "            plt.xlabel('size')\n",
    "            plt.ylabel('count')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        \n",
    "    \n",
    "    if data_save_path is None:\n",
    "        data_save_path = data_path + \"/_resized/\"\n",
    "    for a in age:\n",
    "        for s in sexual:\n",
    "            print(a, s)\n",
    "            path = data_path + '/{}/{}/'.format(a, s)\n",
    "            files = os.listdir(path)\n",
    "            img_save_path = data_save_path + '/{}/{}/'.format(a, s)\n",
    "            if not os.path.exists(img_save_path):\n",
    "                os.makedirs(img_save_path)\n",
    "            for file in files:\n",
    "                if mode == 'L':\n",
    "                    resized_img = misc.imresize(misc.imread(path + file, mode='L'), size=(target_size, target_size, 3)) # bilinear\n",
    "                else:\n",
    "                    resized_img = misc.imresize(misc.imread(path + file), size=(target_size, target_size, 3)) # bilinear\n",
    "                misc.imsave(img_save_path+file, misc.toimage(resized_img/255), )\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult male\n",
      "\n",
      "adult female\n",
      "\n",
      "child male\n",
      "\n",
      "child female\n",
      "\n",
      "elder male\n",
      "\n",
      "elder female\n",
      "\n",
      "young male\n",
      "\n",
      "young female\n",
      "\n"
     ]
    }
   ],
   "source": [
    "change_sizes(data_path='processed_resolve3/', data_save_path='processed_resolve3_resized', target_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part II training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as gbm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_faces_as_dataframe(data_path='processed_reserve3_resized/', size=None):\n",
    "    imgs = []\n",
    "    target = []\n",
    "    for a, i in zip(age, range(len(age))):\n",
    "        for s, j in zip(sexual, range(len(sexual))):\n",
    "            c = i*len(sexual) + j # class from 0 to 7\n",
    "            count = 0\n",
    "            print(a, s)\n",
    "            path = data_path+'{}/{}/'.format(a, s)\n",
    "            files = os.listdir(path)\n",
    "            for file in files:\n",
    "    #             if count > 100:\n",
    "    #                 break\n",
    "                count += 1\n",
    "                if size is None:\n",
    "                    imgs.append(misc.imread(path+file, mode='L').flatten())\n",
    "                else:\n",
    "                    imgs.append(misc.imresize(misc.imread(path+file, mode='L'), size=(size, size)).flatten())\n",
    "                target.append(c)\n",
    "    imgs = np.array(imgs)\n",
    "    target = np.array(target).reshape(len(target), 1)\n",
    "    imgs = np.hstack((imgs, target))\n",
    "    return pd.DataFrame(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult male\n",
      "adult female\n",
      "child male\n",
      "child female\n",
      "elder male\n",
      "elder female\n",
      "young male\n",
      "young female\n"
     ]
    }
   ],
   "source": [
    "size=128\n",
    "df = read_faces_as_dataframe(data_path='processed_resolve3_resized/', size=size)\n",
    "test = df.sample(frac=0.1)\n",
    "train = df.drop(test.index)\n",
    "training = train.values.astype(np.uint8)\n",
    "testing = test.values.astype(np.uint8)\n",
    "np.save('training_grey_rescale_{}'.format(size), training)\n",
    "np.save('testing_grey_rescale_{}'.format(size), testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#df.to_csv('training_data.csv', header=True, index=False) # slow and too big, don't use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult male\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-43fa543e7d72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_faces_as_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_origin{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-b87900d03e6c>\u001b[0m in \u001b[0;36mread_faces_as_dataframe\u001b[0;34m(data_path, size)\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                     \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/scipy/misc/pilutil.py\u001b[0m in \u001b[0;36mimresize\u001b[0;34m(arr, size, interp, mode)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lanczos'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bicubic'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cubic'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m     \u001b[0mimnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfromimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample)\u001b[0m\n\u001b[1;32m   1643\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGBa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGBA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m     def rotate(self, angle, resample=NEAREST, expand=0, center=None,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "size=128\n",
    "df = read_faces_as_dataframe(data_path='data/', size=size)\n",
    "np.save('training_rescale{}'.format(size), df.values.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.load('training_grey_128.npy'))\n",
    "test = df.sample(frac=0.1)\n",
    "train = df.drop(test.index)\n",
    "train_X = train[train.columns[:-1]].values.astype('float32')/255\n",
    "train_t = train[train.columns[-1]].values\n",
    "test_X = test[test.columns[:-1]].values.astype('float32')/255\n",
    "test_t = test[test.columns[-1]].values\n",
    "pca = PCA(50)\n",
    "pca.fit(X=train_X)\n",
    "train_X = pca.fit_transform(train_X)\n",
    "test_X = pca.fit_transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf = gbm.sklearn.LGBMClassifier(n_estimators=100, num_leaves=100, min_child_samples=20,\n",
    "                                 learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113a08c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(pca.explained_variance_)), np.cumsum(pca.explained_variance_))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7384, 4097)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/keras/models.py:240: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "# mothod1, autoencoder\n",
    "import keras\n",
    "encoder = keras.models.load_model('encoder_grey_96')\n",
    "autoencoder = keras.models.load_model('autoencoder_grey_96')\n",
    "\n",
    "img_size=64\n",
    "df = pd.DataFrame(np.load('training_grey_96.npy'))\n",
    "test = df.sample(frac=0.1)\n",
    "train = df.drop(test.index)\n",
    "train_X = train[train.columns[:-1]].values.astype('float32')/255\n",
    "train_t = train[train.columns[-1]].values\n",
    "test_X = test[test.columns[:-1]].values.astype('float32')/255\n",
    "test_t = test[test.columns[-1]].values\n",
    "train_X = encoder.predict(train_X)\n",
    "test_X = encoder.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 2.05347\n",
      "[2]\tvalid_0's multi_logloss: 2.03328\n",
      "[3]\tvalid_0's multi_logloss: 2.01595\n",
      "[4]\tvalid_0's multi_logloss: 2.0019\n",
      "[5]\tvalid_0's multi_logloss: 1.98875\n",
      "[6]\tvalid_0's multi_logloss: 1.97665\n",
      "[7]\tvalid_0's multi_logloss: 1.96645\n",
      "[8]\tvalid_0's multi_logloss: 1.9587\n",
      "[9]\tvalid_0's multi_logloss: 1.94902\n",
      "[10]\tvalid_0's multi_logloss: 1.94016\n",
      "[11]\tvalid_0's multi_logloss: 1.93272\n",
      "[12]\tvalid_0's multi_logloss: 1.9265\n",
      "[13]\tvalid_0's multi_logloss: 1.92106\n",
      "[14]\tvalid_0's multi_logloss: 1.91577\n",
      "[15]\tvalid_0's multi_logloss: 1.91236\n",
      "[16]\tvalid_0's multi_logloss: 1.90883\n",
      "[17]\tvalid_0's multi_logloss: 1.90438\n",
      "[18]\tvalid_0's multi_logloss: 1.90229\n",
      "[19]\tvalid_0's multi_logloss: 1.89995\n",
      "[20]\tvalid_0's multi_logloss: 1.8974\n",
      "[21]\tvalid_0's multi_logloss: 1.89528\n",
      "[22]\tvalid_0's multi_logloss: 1.89337\n",
      "[23]\tvalid_0's multi_logloss: 1.88999\n",
      "[24]\tvalid_0's multi_logloss: 1.88813\n",
      "[25]\tvalid_0's multi_logloss: 1.88635\n",
      "[26]\tvalid_0's multi_logloss: 1.88445\n",
      "[27]\tvalid_0's multi_logloss: 1.88332\n",
      "[28]\tvalid_0's multi_logloss: 1.88264\n",
      "[29]\tvalid_0's multi_logloss: 1.88206\n",
      "[30]\tvalid_0's multi_logloss: 1.88102\n",
      "[31]\tvalid_0's multi_logloss: 1.88009\n",
      "[32]\tvalid_0's multi_logloss: 1.88096\n",
      "[33]\tvalid_0's multi_logloss: 1.88066\n",
      "[34]\tvalid_0's multi_logloss: 1.88066\n",
      "[35]\tvalid_0's multi_logloss: 1.87981\n",
      "[36]\tvalid_0's multi_logloss: 1.87929\n",
      "[37]\tvalid_0's multi_logloss: 1.88105\n",
      "[38]\tvalid_0's multi_logloss: 1.88177\n",
      "[39]\tvalid_0's multi_logloss: 1.88043\n",
      "[40]\tvalid_0's multi_logloss: 1.88022\n",
      "[41]\tvalid_0's multi_logloss: 1.88002\n",
      "[42]\tvalid_0's multi_logloss: 1.87996\n",
      "[43]\tvalid_0's multi_logloss: 1.88082\n",
      "[44]\tvalid_0's multi_logloss: 1.882\n",
      "[45]\tvalid_0's multi_logloss: 1.8833\n",
      "[46]\tvalid_0's multi_logloss: 1.88431\n",
      "[47]\tvalid_0's multi_logloss: 1.88357\n",
      "[48]\tvalid_0's multi_logloss: 1.88475\n",
      "[49]\tvalid_0's multi_logloss: 1.88512\n",
      "[50]\tvalid_0's multi_logloss: 1.884\n",
      "[51]\tvalid_0's multi_logloss: 1.88367\n",
      "[52]\tvalid_0's multi_logloss: 1.88437\n",
      "[53]\tvalid_0's multi_logloss: 1.88395\n",
      "[54]\tvalid_0's multi_logloss: 1.88519\n",
      "[55]\tvalid_0's multi_logloss: 1.88469\n",
      "[56]\tvalid_0's multi_logloss: 1.88512\n",
      "[57]\tvalid_0's multi_logloss: 1.88441\n",
      "[58]\tvalid_0's multi_logloss: 1.88426\n",
      "[59]\tvalid_0's multi_logloss: 1.88563\n",
      "[60]\tvalid_0's multi_logloss: 1.88638\n",
      "[61]\tvalid_0's multi_logloss: 1.88608\n",
      "[62]\tvalid_0's multi_logloss: 1.88632\n",
      "[63]\tvalid_0's multi_logloss: 1.88646\n",
      "[64]\tvalid_0's multi_logloss: 1.88786\n",
      "[65]\tvalid_0's multi_logloss: 1.88834\n",
      "[66]\tvalid_0's multi_logloss: 1.88908\n",
      "[67]\tvalid_0's multi_logloss: 1.88946\n",
      "[68]\tvalid_0's multi_logloss: 1.89046\n",
      "[69]\tvalid_0's multi_logloss: 1.89161\n",
      "[70]\tvalid_0's multi_logloss: 1.89196\n",
      "[71]\tvalid_0's multi_logloss: 1.89307\n",
      "[72]\tvalid_0's multi_logloss: 1.89375\n",
      "[73]\tvalid_0's multi_logloss: 1.89366\n",
      "[74]\tvalid_0's multi_logloss: 1.89441\n",
      "[75]\tvalid_0's multi_logloss: 1.89464\n",
      "[76]\tvalid_0's multi_logloss: 1.8952\n",
      "[77]\tvalid_0's multi_logloss: 1.89668\n",
      "[78]\tvalid_0's multi_logloss: 1.89744\n",
      "[79]\tvalid_0's multi_logloss: 1.89876\n",
      "[80]\tvalid_0's multi_logloss: 1.89941\n",
      "[81]\tvalid_0's multi_logloss: 1.89954\n",
      "[82]\tvalid_0's multi_logloss: 1.9003\n",
      "[83]\tvalid_0's multi_logloss: 1.90098\n",
      "[84]\tvalid_0's multi_logloss: 1.90079\n",
      "[85]\tvalid_0's multi_logloss: 1.9023\n",
      "[86]\tvalid_0's multi_logloss: 1.90135\n",
      "[87]\tvalid_0's multi_logloss: 1.90213\n",
      "[88]\tvalid_0's multi_logloss: 1.90172\n",
      "[89]\tvalid_0's multi_logloss: 1.90195\n",
      "[90]\tvalid_0's multi_logloss: 1.90196\n",
      "[91]\tvalid_0's multi_logloss: 1.90234\n",
      "[92]\tvalid_0's multi_logloss: 1.90407\n",
      "[93]\tvalid_0's multi_logloss: 1.90501\n",
      "[94]\tvalid_0's multi_logloss: 1.9059\n",
      "[95]\tvalid_0's multi_logloss: 1.90584\n",
      "[96]\tvalid_0's multi_logloss: 1.90532\n",
      "[97]\tvalid_0's multi_logloss: 1.9059\n",
      "[98]\tvalid_0's multi_logloss: 1.90668\n",
      "[99]\tvalid_0's multi_logloss: 1.9063\n",
      "[100]\tvalid_0's multi_logloss: 1.90619\n",
      "[101]\tvalid_0's multi_logloss: 1.90609\n",
      "[102]\tvalid_0's multi_logloss: 1.90733\n",
      "[103]\tvalid_0's multi_logloss: 1.90843\n",
      "[104]\tvalid_0's multi_logloss: 1.90938\n",
      "[105]\tvalid_0's multi_logloss: 1.91049\n",
      "[106]\tvalid_0's multi_logloss: 1.91094\n",
      "[107]\tvalid_0's multi_logloss: 1.91108\n",
      "[108]\tvalid_0's multi_logloss: 1.91138\n",
      "[109]\tvalid_0's multi_logloss: 1.91212\n",
      "[110]\tvalid_0's multi_logloss: 1.91294\n",
      "[111]\tvalid_0's multi_logloss: 1.91343\n",
      "[112]\tvalid_0's multi_logloss: 1.91272\n",
      "[113]\tvalid_0's multi_logloss: 1.91363\n",
      "[114]\tvalid_0's multi_logloss: 1.91383\n",
      "[115]\tvalid_0's multi_logloss: 1.91368\n",
      "[116]\tvalid_0's multi_logloss: 1.91303\n",
      "[117]\tvalid_0's multi_logloss: 1.91403\n",
      "[118]\tvalid_0's multi_logloss: 1.91398\n",
      "[119]\tvalid_0's multi_logloss: 1.91526\n",
      "[120]\tvalid_0's multi_logloss: 1.91546\n",
      "[121]\tvalid_0's multi_logloss: 1.91602\n",
      "[122]\tvalid_0's multi_logloss: 1.91693\n",
      "[123]\tvalid_0's multi_logloss: 1.91743\n",
      "[124]\tvalid_0's multi_logloss: 1.91784\n",
      "[125]\tvalid_0's multi_logloss: 1.91877\n",
      "[126]\tvalid_0's multi_logloss: 1.91854\n",
      "[127]\tvalid_0's multi_logloss: 1.91899\n",
      "[128]\tvalid_0's multi_logloss: 1.91975\n",
      "[129]\tvalid_0's multi_logloss: 1.92009\n",
      "[130]\tvalid_0's multi_logloss: 1.92078\n",
      "[131]\tvalid_0's multi_logloss: 1.92115\n",
      "[132]\tvalid_0's multi_logloss: 1.92204\n",
      "[133]\tvalid_0's multi_logloss: 1.92344\n",
      "[134]\tvalid_0's multi_logloss: 1.9247\n",
      "[135]\tvalid_0's multi_logloss: 1.92483\n",
      "[136]\tvalid_0's multi_logloss: 1.92624\n",
      "[137]\tvalid_0's multi_logloss: 1.92605\n",
      "[138]\tvalid_0's multi_logloss: 1.9281\n",
      "[139]\tvalid_0's multi_logloss: 1.92783\n",
      "[140]\tvalid_0's multi_logloss: 1.92994\n",
      "[141]\tvalid_0's multi_logloss: 1.93049\n",
      "[142]\tvalid_0's multi_logloss: 1.93137\n",
      "[143]\tvalid_0's multi_logloss: 1.93216\n",
      "[144]\tvalid_0's multi_logloss: 1.93231\n",
      "[145]\tvalid_0's multi_logloss: 1.93214\n",
      "[146]\tvalid_0's multi_logloss: 1.93316\n",
      "[147]\tvalid_0's multi_logloss: 1.93377\n",
      "[148]\tvalid_0's multi_logloss: 1.93469\n",
      "[149]\tvalid_0's multi_logloss: 1.93508\n",
      "[150]\tvalid_0's multi_logloss: 1.93646\n",
      "[151]\tvalid_0's multi_logloss: 1.93689\n",
      "[152]\tvalid_0's multi_logloss: 1.9374\n",
      "[153]\tvalid_0's multi_logloss: 1.93866\n",
      "[154]\tvalid_0's multi_logloss: 1.93891\n",
      "[155]\tvalid_0's multi_logloss: 1.94054\n",
      "[156]\tvalid_0's multi_logloss: 1.94171\n",
      "[157]\tvalid_0's multi_logloss: 1.94278\n",
      "[158]\tvalid_0's multi_logloss: 1.94423\n",
      "[159]\tvalid_0's multi_logloss: 1.94405\n",
      "[160]\tvalid_0's multi_logloss: 1.94457\n",
      "[161]\tvalid_0's multi_logloss: 1.94619\n",
      "[162]\tvalid_0's multi_logloss: 1.94592\n",
      "[163]\tvalid_0's multi_logloss: 1.94747\n",
      "[164]\tvalid_0's multi_logloss: 1.94807\n",
      "[165]\tvalid_0's multi_logloss: 1.94974\n",
      "[166]\tvalid_0's multi_logloss: 1.95027\n",
      "[167]\tvalid_0's multi_logloss: 1.95136\n",
      "[168]\tvalid_0's multi_logloss: 1.95189\n",
      "[169]\tvalid_0's multi_logloss: 1.95301\n",
      "[170]\tvalid_0's multi_logloss: 1.95316\n",
      "[171]\tvalid_0's multi_logloss: 1.95478\n",
      "[172]\tvalid_0's multi_logloss: 1.95502\n",
      "[173]\tvalid_0's multi_logloss: 1.95595\n",
      "[174]\tvalid_0's multi_logloss: 1.95653\n",
      "[175]\tvalid_0's multi_logloss: 1.95745\n",
      "[176]\tvalid_0's multi_logloss: 1.95863\n",
      "[177]\tvalid_0's multi_logloss: 1.95955\n",
      "[178]\tvalid_0's multi_logloss: 1.9603\n",
      "[179]\tvalid_0's multi_logloss: 1.9615\n",
      "[180]\tvalid_0's multi_logloss: 1.96243\n",
      "[181]\tvalid_0's multi_logloss: 1.9626\n",
      "[182]\tvalid_0's multi_logloss: 1.96327\n",
      "[183]\tvalid_0's multi_logloss: 1.96412\n",
      "[184]\tvalid_0's multi_logloss: 1.96466\n",
      "[185]\tvalid_0's multi_logloss: 1.96608\n",
      "[186]\tvalid_0's multi_logloss: 1.96662\n",
      "[187]\tvalid_0's multi_logloss: 1.96661\n",
      "[188]\tvalid_0's multi_logloss: 1.96686\n",
      "[189]\tvalid_0's multi_logloss: 1.967\n",
      "[190]\tvalid_0's multi_logloss: 1.96745\n",
      "[191]\tvalid_0's multi_logloss: 1.96823\n",
      "[192]\tvalid_0's multi_logloss: 1.96843\n",
      "[193]\tvalid_0's multi_logloss: 1.96908\n",
      "[194]\tvalid_0's multi_logloss: 1.96912\n",
      "[195]\tvalid_0's multi_logloss: 1.9693\n",
      "[196]\tvalid_0's multi_logloss: 1.96931\n",
      "[197]\tvalid_0's multi_logloss: 1.97064\n",
      "[198]\tvalid_0's multi_logloss: 1.97138\n",
      "[199]\tvalid_0's multi_logloss: 1.97266\n",
      "[200]\tvalid_0's multi_logloss: 1.97369\n",
      "[201]\tvalid_0's multi_logloss: 1.97396\n",
      "[202]\tvalid_0's multi_logloss: 1.9745\n",
      "[203]\tvalid_0's multi_logloss: 1.9751\n",
      "[204]\tvalid_0's multi_logloss: 1.97595\n",
      "[205]\tvalid_0's multi_logloss: 1.97718\n",
      "[206]\tvalid_0's multi_logloss: 1.97774\n",
      "[207]\tvalid_0's multi_logloss: 1.9782\n",
      "[208]\tvalid_0's multi_logloss: 1.97844\n",
      "[209]\tvalid_0's multi_logloss: 1.97942\n",
      "[210]\tvalid_0's multi_logloss: 1.98055\n",
      "[211]\tvalid_0's multi_logloss: 1.98054\n",
      "[212]\tvalid_0's multi_logloss: 1.98129\n",
      "[213]\tvalid_0's multi_logloss: 1.98264\n",
      "[214]\tvalid_0's multi_logloss: 1.9839\n",
      "[215]\tvalid_0's multi_logloss: 1.9844\n",
      "[216]\tvalid_0's multi_logloss: 1.98536\n",
      "[217]\tvalid_0's multi_logloss: 1.98605\n",
      "[218]\tvalid_0's multi_logloss: 1.98683\n",
      "[219]\tvalid_0's multi_logloss: 1.98694\n",
      "[220]\tvalid_0's multi_logloss: 1.98817\n",
      "[221]\tvalid_0's multi_logloss: 1.9888\n",
      "[222]\tvalid_0's multi_logloss: 1.98895\n",
      "[223]\tvalid_0's multi_logloss: 1.98864\n",
      "[224]\tvalid_0's multi_logloss: 1.9897\n",
      "[225]\tvalid_0's multi_logloss: 1.99029\n",
      "[226]\tvalid_0's multi_logloss: 1.99047\n",
      "[227]\tvalid_0's multi_logloss: 1.99154\n",
      "[228]\tvalid_0's multi_logloss: 1.99209\n",
      "[229]\tvalid_0's multi_logloss: 1.9931\n",
      "[230]\tvalid_0's multi_logloss: 1.99388\n",
      "[231]\tvalid_0's multi_logloss: 1.99479\n",
      "[232]\tvalid_0's multi_logloss: 1.9949\n",
      "[233]\tvalid_0's multi_logloss: 1.99545\n",
      "[234]\tvalid_0's multi_logloss: 1.99642\n",
      "[235]\tvalid_0's multi_logloss: 1.99677\n",
      "[236]\tvalid_0's multi_logloss: 1.99715\n",
      "[237]\tvalid_0's multi_logloss: 1.99813\n",
      "[238]\tvalid_0's multi_logloss: 1.99844\n",
      "[239]\tvalid_0's multi_logloss: 1.99868\n",
      "[240]\tvalid_0's multi_logloss: 1.99953\n",
      "[241]\tvalid_0's multi_logloss: 2.00029\n",
      "[242]\tvalid_0's multi_logloss: 2.00081\n",
      "[243]\tvalid_0's multi_logloss: 2.0016\n",
      "[244]\tvalid_0's multi_logloss: 2.00237\n",
      "[245]\tvalid_0's multi_logloss: 2.00293\n",
      "[246]\tvalid_0's multi_logloss: 2.00336\n",
      "[247]\tvalid_0's multi_logloss: 2.00377\n",
      "[248]\tvalid_0's multi_logloss: 2.00456\n",
      "[249]\tvalid_0's multi_logloss: 2.0052\n",
      "[250]\tvalid_0's multi_logloss: 2.0067\n",
      "[251]\tvalid_0's multi_logloss: 2.00716\n",
      "[252]\tvalid_0's multi_logloss: 2.00815\n",
      "[253]\tvalid_0's multi_logloss: 2.00835\n",
      "[254]\tvalid_0's multi_logloss: 2.00911\n",
      "[255]\tvalid_0's multi_logloss: 2.00812\n",
      "[256]\tvalid_0's multi_logloss: 2.00877\n",
      "[257]\tvalid_0's multi_logloss: 2.00902\n",
      "[258]\tvalid_0's multi_logloss: 2.00921\n",
      "[259]\tvalid_0's multi_logloss: 2.01038\n",
      "[260]\tvalid_0's multi_logloss: 2.01165\n",
      "[261]\tvalid_0's multi_logloss: 2.01183\n",
      "[262]\tvalid_0's multi_logloss: 2.012\n",
      "[263]\tvalid_0's multi_logloss: 2.0128\n",
      "[264]\tvalid_0's multi_logloss: 2.01294\n",
      "[265]\tvalid_0's multi_logloss: 2.01297\n",
      "[266]\tvalid_0's multi_logloss: 2.01323\n",
      "[267]\tvalid_0's multi_logloss: 2.01353\n",
      "[268]\tvalid_0's multi_logloss: 2.01366\n",
      "[269]\tvalid_0's multi_logloss: 2.0144\n",
      "[270]\tvalid_0's multi_logloss: 2.01463\n",
      "[271]\tvalid_0's multi_logloss: 2.01544\n",
      "[272]\tvalid_0's multi_logloss: 2.01603\n",
      "[273]\tvalid_0's multi_logloss: 2.01615\n",
      "[274]\tvalid_0's multi_logloss: 2.01704\n",
      "[275]\tvalid_0's multi_logloss: 2.01762\n",
      "[276]\tvalid_0's multi_logloss: 2.01723\n",
      "[277]\tvalid_0's multi_logloss: 2.01849\n",
      "[278]\tvalid_0's multi_logloss: 2.01969\n",
      "[279]\tvalid_0's multi_logloss: 2.02029\n",
      "[280]\tvalid_0's multi_logloss: 2.02054\n",
      "[281]\tvalid_0's multi_logloss: 2.02171\n",
      "[282]\tvalid_0's multi_logloss: 2.02234\n",
      "[283]\tvalid_0's multi_logloss: 2.02288\n",
      "[284]\tvalid_0's multi_logloss: 2.02423\n",
      "[285]\tvalid_0's multi_logloss: 2.02552\n",
      "[286]\tvalid_0's multi_logloss: 2.02712\n",
      "[287]\tvalid_0's multi_logloss: 2.02746\n",
      "[288]\tvalid_0's multi_logloss: 2.02791\n",
      "[289]\tvalid_0's multi_logloss: 2.02869\n",
      "[290]\tvalid_0's multi_logloss: 2.02957\n",
      "[291]\tvalid_0's multi_logloss: 2.02967\n",
      "[292]\tvalid_0's multi_logloss: 2.03067\n",
      "[293]\tvalid_0's multi_logloss: 2.03106\n",
      "[294]\tvalid_0's multi_logloss: 2.03248\n",
      "[295]\tvalid_0's multi_logloss: 2.03443\n",
      "[296]\tvalid_0's multi_logloss: 2.03534\n",
      "[297]\tvalid_0's multi_logloss: 2.0364\n",
      "[298]\tvalid_0's multi_logloss: 2.03701\n",
      "[299]\tvalid_0's multi_logloss: 2.03733\n",
      "[300]\tvalid_0's multi_logloss: 2.03798\n",
      "[301]\tvalid_0's multi_logloss: 2.03814\n",
      "[302]\tvalid_0's multi_logloss: 2.03935\n",
      "[303]\tvalid_0's multi_logloss: 2.03938\n",
      "[304]\tvalid_0's multi_logloss: 2.0406\n",
      "[305]\tvalid_0's multi_logloss: 2.04127\n",
      "[306]\tvalid_0's multi_logloss: 2.04184\n",
      "[307]\tvalid_0's multi_logloss: 2.04326\n",
      "[308]\tvalid_0's multi_logloss: 2.04449\n",
      "[309]\tvalid_0's multi_logloss: 2.04429\n",
      "[310]\tvalid_0's multi_logloss: 2.04569\n",
      "[311]\tvalid_0's multi_logloss: 2.04618\n",
      "[312]\tvalid_0's multi_logloss: 2.0474\n",
      "[313]\tvalid_0's multi_logloss: 2.04737\n",
      "[314]\tvalid_0's multi_logloss: 2.04826\n",
      "[315]\tvalid_0's multi_logloss: 2.04994\n",
      "[316]\tvalid_0's multi_logloss: 2.05083\n",
      "[317]\tvalid_0's multi_logloss: 2.05226\n",
      "[318]\tvalid_0's multi_logloss: 2.05218\n",
      "[319]\tvalid_0's multi_logloss: 2.05177\n",
      "[320]\tvalid_0's multi_logloss: 2.05252\n",
      "[321]\tvalid_0's multi_logloss: 2.05391\n",
      "[322]\tvalid_0's multi_logloss: 2.05457\n",
      "[323]\tvalid_0's multi_logloss: 2.05505\n",
      "[324]\tvalid_0's multi_logloss: 2.05679\n",
      "[325]\tvalid_0's multi_logloss: 2.05722\n",
      "[326]\tvalid_0's multi_logloss: 2.05746\n",
      "[327]\tvalid_0's multi_logloss: 2.05835\n",
      "[328]\tvalid_0's multi_logloss: 2.05942\n",
      "[329]\tvalid_0's multi_logloss: 2.06031\n",
      "[330]\tvalid_0's multi_logloss: 2.06055\n",
      "[331]\tvalid_0's multi_logloss: 2.06116\n",
      "[332]\tvalid_0's multi_logloss: 2.06169\n",
      "[333]\tvalid_0's multi_logloss: 2.06172\n",
      "[334]\tvalid_0's multi_logloss: 2.06208\n",
      "[335]\tvalid_0's multi_logloss: 2.06271\n",
      "[336]\tvalid_0's multi_logloss: 2.06373\n",
      "[337]\tvalid_0's multi_logloss: 2.06431\n",
      "[338]\tvalid_0's multi_logloss: 2.06394\n",
      "[339]\tvalid_0's multi_logloss: 2.06406\n",
      "[340]\tvalid_0's multi_logloss: 2.0646\n",
      "[341]\tvalid_0's multi_logloss: 2.06477\n",
      "[342]\tvalid_0's multi_logloss: 2.06464\n",
      "[343]\tvalid_0's multi_logloss: 2.0651\n",
      "[344]\tvalid_0's multi_logloss: 2.06598\n",
      "[345]\tvalid_0's multi_logloss: 2.06584\n",
      "[346]\tvalid_0's multi_logloss: 2.06662\n",
      "[347]\tvalid_0's multi_logloss: 2.0674\n",
      "[348]\tvalid_0's multi_logloss: 2.06874\n",
      "[349]\tvalid_0's multi_logloss: 2.06974\n",
      "[350]\tvalid_0's multi_logloss: 2.07062\n",
      "[351]\tvalid_0's multi_logloss: 2.07075\n",
      "[352]\tvalid_0's multi_logloss: 2.07124\n",
      "[353]\tvalid_0's multi_logloss: 2.07142\n",
      "[354]\tvalid_0's multi_logloss: 2.07204\n",
      "[355]\tvalid_0's multi_logloss: 2.07283\n",
      "[356]\tvalid_0's multi_logloss: 2.07369\n",
      "[357]\tvalid_0's multi_logloss: 2.07443\n",
      "[358]\tvalid_0's multi_logloss: 2.07499\n",
      "[359]\tvalid_0's multi_logloss: 2.07559\n",
      "[360]\tvalid_0's multi_logloss: 2.07627\n",
      "[361]\tvalid_0's multi_logloss: 2.07798\n",
      "[362]\tvalid_0's multi_logloss: 2.07989\n",
      "[363]\tvalid_0's multi_logloss: 2.08059\n",
      "[364]\tvalid_0's multi_logloss: 2.08117\n",
      "[365]\tvalid_0's multi_logloss: 2.08212\n",
      "[366]\tvalid_0's multi_logloss: 2.08272\n",
      "[367]\tvalid_0's multi_logloss: 2.08402\n",
      "[368]\tvalid_0's multi_logloss: 2.08432\n",
      "[369]\tvalid_0's multi_logloss: 2.08458\n",
      "[370]\tvalid_0's multi_logloss: 2.08489\n",
      "[371]\tvalid_0's multi_logloss: 2.08508\n",
      "[372]\tvalid_0's multi_logloss: 2.08534\n",
      "[373]\tvalid_0's multi_logloss: 2.08548\n",
      "[374]\tvalid_0's multi_logloss: 2.08706\n",
      "[375]\tvalid_0's multi_logloss: 2.08748\n",
      "[376]\tvalid_0's multi_logloss: 2.08777\n",
      "[377]\tvalid_0's multi_logloss: 2.08842\n",
      "[378]\tvalid_0's multi_logloss: 2.08874\n",
      "[379]\tvalid_0's multi_logloss: 2.08936\n",
      "[380]\tvalid_0's multi_logloss: 2.09039\n",
      "[381]\tvalid_0's multi_logloss: 2.0909\n",
      "[382]\tvalid_0's multi_logloss: 2.09168\n",
      "[383]\tvalid_0's multi_logloss: 2.09246\n",
      "[384]\tvalid_0's multi_logloss: 2.09369\n",
      "[385]\tvalid_0's multi_logloss: 2.09427\n",
      "[386]\tvalid_0's multi_logloss: 2.0952\n",
      "[387]\tvalid_0's multi_logloss: 2.09601\n",
      "[388]\tvalid_0's multi_logloss: 2.09751\n",
      "[389]\tvalid_0's multi_logloss: 2.09825\n",
      "[390]\tvalid_0's multi_logloss: 2.09961\n",
      "[391]\tvalid_0's multi_logloss: 2.10059\n",
      "[392]\tvalid_0's multi_logloss: 2.10123\n",
      "[393]\tvalid_0's multi_logloss: 2.1018\n",
      "[394]\tvalid_0's multi_logloss: 2.10266\n",
      "[395]\tvalid_0's multi_logloss: 2.1034\n",
      "[396]\tvalid_0's multi_logloss: 2.10409\n",
      "[397]\tvalid_0's multi_logloss: 2.10435\n",
      "[398]\tvalid_0's multi_logloss: 2.10461\n",
      "[399]\tvalid_0's multi_logloss: 2.10536\n",
      "[400]\tvalid_0's multi_logloss: 2.10688\n",
      "[401]\tvalid_0's multi_logloss: 2.10753\n",
      "[402]\tvalid_0's multi_logloss: 2.10837\n",
      "[403]\tvalid_0's multi_logloss: 2.10901\n",
      "[404]\tvalid_0's multi_logloss: 2.10952\n",
      "[405]\tvalid_0's multi_logloss: 2.11019\n",
      "[406]\tvalid_0's multi_logloss: 2.11106\n",
      "[407]\tvalid_0's multi_logloss: 2.11157\n",
      "[408]\tvalid_0's multi_logloss: 2.11305\n",
      "[409]\tvalid_0's multi_logloss: 2.11337\n",
      "[410]\tvalid_0's multi_logloss: 2.11318\n",
      "[411]\tvalid_0's multi_logloss: 2.11389\n",
      "[412]\tvalid_0's multi_logloss: 2.11437\n",
      "[413]\tvalid_0's multi_logloss: 2.1149\n",
      "[414]\tvalid_0's multi_logloss: 2.11603\n",
      "[415]\tvalid_0's multi_logloss: 2.11672\n",
      "[416]\tvalid_0's multi_logloss: 2.11684\n",
      "[417]\tvalid_0's multi_logloss: 2.11774\n",
      "[418]\tvalid_0's multi_logloss: 2.11903\n",
      "[419]\tvalid_0's multi_logloss: 2.11887\n",
      "[420]\tvalid_0's multi_logloss: 2.11944\n",
      "[421]\tvalid_0's multi_logloss: 2.11984\n",
      "[422]\tvalid_0's multi_logloss: 2.12089\n",
      "[423]\tvalid_0's multi_logloss: 2.12096\n",
      "[424]\tvalid_0's multi_logloss: 2.12185\n",
      "[425]\tvalid_0's multi_logloss: 2.12185\n",
      "[426]\tvalid_0's multi_logloss: 2.12277\n",
      "[427]\tvalid_0's multi_logloss: 2.12376\n",
      "[428]\tvalid_0's multi_logloss: 2.12454\n",
      "[429]\tvalid_0's multi_logloss: 2.12508\n",
      "[430]\tvalid_0's multi_logloss: 2.12568\n",
      "[431]\tvalid_0's multi_logloss: 2.12666\n",
      "[432]\tvalid_0's multi_logloss: 2.12785\n",
      "[433]\tvalid_0's multi_logloss: 2.12808\n",
      "[434]\tvalid_0's multi_logloss: 2.1282\n",
      "[435]\tvalid_0's multi_logloss: 2.1289\n",
      "[436]\tvalid_0's multi_logloss: 2.12944\n",
      "[437]\tvalid_0's multi_logloss: 2.13031\n",
      "[438]\tvalid_0's multi_logloss: 2.13092\n",
      "[439]\tvalid_0's multi_logloss: 2.13119\n",
      "[440]\tvalid_0's multi_logloss: 2.13158\n",
      "[441]\tvalid_0's multi_logloss: 2.13157\n",
      "[442]\tvalid_0's multi_logloss: 2.13296\n",
      "[443]\tvalid_0's multi_logloss: 2.1336\n",
      "[444]\tvalid_0's multi_logloss: 2.13463\n",
      "[445]\tvalid_0's multi_logloss: 2.1354\n",
      "[446]\tvalid_0's multi_logloss: 2.13576\n",
      "[447]\tvalid_0's multi_logloss: 2.13603\n",
      "[448]\tvalid_0's multi_logloss: 2.13624\n",
      "[449]\tvalid_0's multi_logloss: 2.13679\n",
      "[450]\tvalid_0's multi_logloss: 2.13733\n",
      "[451]\tvalid_0's multi_logloss: 2.13827\n",
      "[452]\tvalid_0's multi_logloss: 2.13835\n",
      "[453]\tvalid_0's multi_logloss: 2.1384\n",
      "[454]\tvalid_0's multi_logloss: 2.13843\n",
      "[455]\tvalid_0's multi_logloss: 2.13939\n",
      "[456]\tvalid_0's multi_logloss: 2.13991\n",
      "[457]\tvalid_0's multi_logloss: 2.14084\n",
      "[458]\tvalid_0's multi_logloss: 2.14133\n",
      "[459]\tvalid_0's multi_logloss: 2.14184\n",
      "[460]\tvalid_0's multi_logloss: 2.14264\n",
      "[461]\tvalid_0's multi_logloss: 2.14294\n",
      "[462]\tvalid_0's multi_logloss: 2.14354\n",
      "[463]\tvalid_0's multi_logloss: 2.14373\n",
      "[464]\tvalid_0's multi_logloss: 2.14422\n",
      "[465]\tvalid_0's multi_logloss: 2.14482\n",
      "[466]\tvalid_0's multi_logloss: 2.14493\n",
      "[467]\tvalid_0's multi_logloss: 2.14579\n",
      "[468]\tvalid_0's multi_logloss: 2.14652\n",
      "[469]\tvalid_0's multi_logloss: 2.14763\n",
      "[470]\tvalid_0's multi_logloss: 2.14815\n",
      "[471]\tvalid_0's multi_logloss: 2.1492\n",
      "[472]\tvalid_0's multi_logloss: 2.14962\n",
      "[473]\tvalid_0's multi_logloss: 2.15036\n",
      "[474]\tvalid_0's multi_logloss: 2.15078\n",
      "[475]\tvalid_0's multi_logloss: 2.15155\n",
      "[476]\tvalid_0's multi_logloss: 2.15258\n",
      "[477]\tvalid_0's multi_logloss: 2.1536\n",
      "[478]\tvalid_0's multi_logloss: 2.1543\n",
      "[479]\tvalid_0's multi_logloss: 2.15474\n",
      "[480]\tvalid_0's multi_logloss: 2.15523\n",
      "[481]\tvalid_0's multi_logloss: 2.15592\n",
      "[482]\tvalid_0's multi_logloss: 2.15678\n",
      "[483]\tvalid_0's multi_logloss: 2.15706\n",
      "[484]\tvalid_0's multi_logloss: 2.15745\n",
      "[485]\tvalid_0's multi_logloss: 2.15921\n",
      "[486]\tvalid_0's multi_logloss: 2.15981\n",
      "[487]\tvalid_0's multi_logloss: 2.1602\n",
      "[488]\tvalid_0's multi_logloss: 2.16092\n",
      "[489]\tvalid_0's multi_logloss: 2.16154\n",
      "[490]\tvalid_0's multi_logloss: 2.1629\n",
      "[491]\tvalid_0's multi_logloss: 2.16389\n",
      "[492]\tvalid_0's multi_logloss: 2.16432\n",
      "[493]\tvalid_0's multi_logloss: 2.16528\n",
      "[494]\tvalid_0's multi_logloss: 2.16588\n",
      "[495]\tvalid_0's multi_logloss: 2.16652\n",
      "[496]\tvalid_0's multi_logloss: 2.16745\n",
      "[497]\tvalid_0's multi_logloss: 2.16827\n",
      "[498]\tvalid_0's multi_logloss: 2.16887\n",
      "[499]\tvalid_0's multi_logloss: 2.16888\n",
      "[500]\tvalid_0's multi_logloss: 2.16928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=1, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=255, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=20, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=500, nthread=-1, num_leaves=20,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = gbm.sklearn.LGBMClassifier(n_estimators=500, num_leaves=20, min_child_samples=50)\n",
    "clf.fit(train_X, train_t, eval_set=(test_X, test_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_result = clf.predict(train_X)\n",
    "test_result = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_acc = len(train_t[train_result == train_t]) / len(train_t)\n",
    "test_acc = len(test_t[test_result == test_t]) / len(test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9474220501324638 0.25321100917431194\n"
     ]
    }
   ],
   "source": [
    "print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(columns=['type_of_AE', 'input_size', 'reserved_all', 'trees', 'acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. convolution auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6646, 16384)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/keras/models.py:240: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected input_1 to have 4 dimensions, but got array with shape (6646, 16384)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-83caf381ab98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# encode and decode some digits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#encoder = Model(input_img, encoded)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtest_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1552\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1553\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    119\u001b[0m                                  \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                                  \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected input_1 to have 4 dimensions, but got array with shape (6646, 16384)"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "encoder = keras.models.load_model('conv_encoder_resolve_128')\n",
    "autoencoder = keras.models.load_model('conv_autoencoder_resolve_128')\n",
    "\n",
    "img_size = 128\n",
    "train = pd.DataFrame(np.load('training_grey_rescale_128.npy'))\n",
    "train_X = train[train.columns[:-1]].values.astype('float32')/255\n",
    "train_t = train[train.columns[-1]].values\n",
    "test = pd.DataFrame(np.load('testing_grey_rescale_128.npy'))\n",
    "test_X = test[test.columns[:-1]].values.astype('float32')/255\n",
    "test_t = test[test.columns[-1]].values\n",
    "\n",
    "# encode and decode some digits\n",
    "#encoder = Model(input_img, encoded)\n",
    "train_X = encoder.predict(train_X)\n",
    "test_X = encoder.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/keras/models.py:240: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "encoder = keras.models.load_model('conv_encoder_resolve_128')\n",
    "autoencoder = keras.models.load_model('conv_autoencoder_resolve_128')\n",
    "\n",
    "img_size = 128\n",
    "train = pd.DataFrame(np.load('training_grey_rescale_128.npy'))\n",
    "train_X = train[train.columns[:-1]].values.astype('float32')/255\n",
    "train_t = train[train.columns[-1]].values\n",
    "test = pd.DataFrame(np.load('testing_grey_rescale_128.npy'))\n",
    "test_X = test[test.columns[:-1]].values.astype('float32')/255\n",
    "test_t = test[test.columns[-1]].values\n",
    "\n",
    "# encode and decode some digits\n",
    "#encoder = Model(input_img, encoded)\n",
    "train_X = encoder.predict(train_X.reshape((len(train_X), img_size, img_size, 1)))\n",
    "test_X = encoder.predict(test_X.reshape((len(test_X), img_size, img_size, 1)))\n",
    "dim = train_X.shape[1] * train_X.shape[2] * train_X.shape[3]\n",
    "train_X = train_X.reshape((len(train_X), dim))\n",
    "test_X = test_X.reshape((len(test_X), dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 2.05459\n",
      "[2]\tvalid_0's multi_logloss: 2.03364\n",
      "[3]\tvalid_0's multi_logloss: 2.01459\n",
      "[4]\tvalid_0's multi_logloss: 1.995\n",
      "[5]\tvalid_0's multi_logloss: 1.97808\n",
      "[6]\tvalid_0's multi_logloss: 1.96281\n",
      "[7]\tvalid_0's multi_logloss: 1.94707\n",
      "[8]\tvalid_0's multi_logloss: 1.93315\n",
      "[9]\tvalid_0's multi_logloss: 1.92021\n",
      "[10]\tvalid_0's multi_logloss: 1.91061\n",
      "[11]\tvalid_0's multi_logloss: 1.89765\n",
      "[12]\tvalid_0's multi_logloss: 1.88874\n",
      "[13]\tvalid_0's multi_logloss: 1.88051\n",
      "[14]\tvalid_0's multi_logloss: 1.87115\n",
      "[15]\tvalid_0's multi_logloss: 1.86318\n",
      "[16]\tvalid_0's multi_logloss: 1.85663\n",
      "[17]\tvalid_0's multi_logloss: 1.85013\n",
      "[18]\tvalid_0's multi_logloss: 1.84422\n",
      "[19]\tvalid_0's multi_logloss: 1.83827\n",
      "[20]\tvalid_0's multi_logloss: 1.83299\n",
      "[21]\tvalid_0's multi_logloss: 1.82743\n",
      "[22]\tvalid_0's multi_logloss: 1.82309\n",
      "[23]\tvalid_0's multi_logloss: 1.81996\n",
      "[24]\tvalid_0's multi_logloss: 1.81646\n",
      "[25]\tvalid_0's multi_logloss: 1.81455\n",
      "[26]\tvalid_0's multi_logloss: 1.80937\n",
      "[27]\tvalid_0's multi_logloss: 1.806\n",
      "[28]\tvalid_0's multi_logloss: 1.80036\n",
      "[29]\tvalid_0's multi_logloss: 1.79609\n",
      "[30]\tvalid_0's multi_logloss: 1.79228\n",
      "[31]\tvalid_0's multi_logloss: 1.7891\n",
      "[32]\tvalid_0's multi_logloss: 1.785\n",
      "[33]\tvalid_0's multi_logloss: 1.78011\n",
      "[34]\tvalid_0's multi_logloss: 1.77651\n",
      "[35]\tvalid_0's multi_logloss: 1.77329\n",
      "[36]\tvalid_0's multi_logloss: 1.76989\n",
      "[37]\tvalid_0's multi_logloss: 1.76619\n",
      "[38]\tvalid_0's multi_logloss: 1.76332\n",
      "[39]\tvalid_0's multi_logloss: 1.75925\n",
      "[40]\tvalid_0's multi_logloss: 1.75736\n",
      "[41]\tvalid_0's multi_logloss: 1.75286\n",
      "[42]\tvalid_0's multi_logloss: 1.75059\n",
      "[43]\tvalid_0's multi_logloss: 1.74778\n",
      "[44]\tvalid_0's multi_logloss: 1.74382\n",
      "[45]\tvalid_0's multi_logloss: 1.74176\n",
      "[46]\tvalid_0's multi_logloss: 1.73917\n",
      "[47]\tvalid_0's multi_logloss: 1.73696\n",
      "[48]\tvalid_0's multi_logloss: 1.7339\n",
      "[49]\tvalid_0's multi_logloss: 1.7321\n",
      "[50]\tvalid_0's multi_logloss: 1.72932\n",
      "[51]\tvalid_0's multi_logloss: 1.72727\n",
      "[52]\tvalid_0's multi_logloss: 1.72547\n",
      "[53]\tvalid_0's multi_logloss: 1.72462\n",
      "[54]\tvalid_0's multi_logloss: 1.7226\n",
      "[55]\tvalid_0's multi_logloss: 1.71978\n",
      "[56]\tvalid_0's multi_logloss: 1.71775\n",
      "[57]\tvalid_0's multi_logloss: 1.71454\n",
      "[58]\tvalid_0's multi_logloss: 1.71344\n",
      "[59]\tvalid_0's multi_logloss: 1.71133\n",
      "[60]\tvalid_0's multi_logloss: 1.70966\n",
      "[61]\tvalid_0's multi_logloss: 1.70795\n",
      "[62]\tvalid_0's multi_logloss: 1.70605\n",
      "[63]\tvalid_0's multi_logloss: 1.70554\n",
      "[64]\tvalid_0's multi_logloss: 1.7032\n",
      "[65]\tvalid_0's multi_logloss: 1.70186\n",
      "[66]\tvalid_0's multi_logloss: 1.70003\n",
      "[67]\tvalid_0's multi_logloss: 1.6979\n",
      "[68]\tvalid_0's multi_logloss: 1.69646\n",
      "[69]\tvalid_0's multi_logloss: 1.69465\n",
      "[70]\tvalid_0's multi_logloss: 1.69408\n",
      "[71]\tvalid_0's multi_logloss: 1.69119\n",
      "[72]\tvalid_0's multi_logloss: 1.68921\n",
      "[73]\tvalid_0's multi_logloss: 1.68841\n",
      "[74]\tvalid_0's multi_logloss: 1.68682\n",
      "[75]\tvalid_0's multi_logloss: 1.68582\n",
      "[76]\tvalid_0's multi_logloss: 1.6845\n",
      "[77]\tvalid_0's multi_logloss: 1.68427\n",
      "[78]\tvalid_0's multi_logloss: 1.68173\n",
      "[79]\tvalid_0's multi_logloss: 1.67978\n",
      "[80]\tvalid_0's multi_logloss: 1.67922\n",
      "[81]\tvalid_0's multi_logloss: 1.67837\n",
      "[82]\tvalid_0's multi_logloss: 1.67837\n",
      "[83]\tvalid_0's multi_logloss: 1.67736\n",
      "[84]\tvalid_0's multi_logloss: 1.6765\n",
      "[85]\tvalid_0's multi_logloss: 1.67615\n",
      "[86]\tvalid_0's multi_logloss: 1.67501\n",
      "[87]\tvalid_0's multi_logloss: 1.67289\n",
      "[88]\tvalid_0's multi_logloss: 1.67092\n",
      "[89]\tvalid_0's multi_logloss: 1.66986\n",
      "[90]\tvalid_0's multi_logloss: 1.66943\n",
      "[91]\tvalid_0's multi_logloss: 1.66806\n",
      "[92]\tvalid_0's multi_logloss: 1.66686\n",
      "[93]\tvalid_0's multi_logloss: 1.66474\n",
      "[94]\tvalid_0's multi_logloss: 1.66427\n",
      "[95]\tvalid_0's multi_logloss: 1.66264\n",
      "[96]\tvalid_0's multi_logloss: 1.66047\n",
      "[97]\tvalid_0's multi_logloss: 1.65887\n",
      "[98]\tvalid_0's multi_logloss: 1.65918\n",
      "[99]\tvalid_0's multi_logloss: 1.65852\n",
      "[100]\tvalid_0's multi_logloss: 1.65755\n",
      "[101]\tvalid_0's multi_logloss: 1.65716\n",
      "[102]\tvalid_0's multi_logloss: 1.6557\n",
      "[103]\tvalid_0's multi_logloss: 1.65447\n",
      "[104]\tvalid_0's multi_logloss: 1.65329\n",
      "[105]\tvalid_0's multi_logloss: 1.65229\n",
      "[106]\tvalid_0's multi_logloss: 1.65239\n",
      "[107]\tvalid_0's multi_logloss: 1.65145\n",
      "[108]\tvalid_0's multi_logloss: 1.65025\n",
      "[109]\tvalid_0's multi_logloss: 1.64988\n",
      "[110]\tvalid_0's multi_logloss: 1.64888\n",
      "[111]\tvalid_0's multi_logloss: 1.64855\n",
      "[112]\tvalid_0's multi_logloss: 1.64709\n",
      "[113]\tvalid_0's multi_logloss: 1.64601\n",
      "[114]\tvalid_0's multi_logloss: 1.64563\n",
      "[115]\tvalid_0's multi_logloss: 1.64435\n",
      "[116]\tvalid_0's multi_logloss: 1.64344\n",
      "[117]\tvalid_0's multi_logloss: 1.6426\n",
      "[118]\tvalid_0's multi_logloss: 1.64175\n",
      "[119]\tvalid_0's multi_logloss: 1.6421\n",
      "[120]\tvalid_0's multi_logloss: 1.64254\n",
      "[121]\tvalid_0's multi_logloss: 1.64245\n",
      "[122]\tvalid_0's multi_logloss: 1.64218\n",
      "[123]\tvalid_0's multi_logloss: 1.64218\n",
      "[124]\tvalid_0's multi_logloss: 1.64127\n",
      "[125]\tvalid_0's multi_logloss: 1.64084\n",
      "[126]\tvalid_0's multi_logloss: 1.64158\n",
      "[127]\tvalid_0's multi_logloss: 1.64114\n",
      "[128]\tvalid_0's multi_logloss: 1.64121\n",
      "[129]\tvalid_0's multi_logloss: 1.63986\n",
      "[130]\tvalid_0's multi_logloss: 1.63864\n",
      "[131]\tvalid_0's multi_logloss: 1.63908\n",
      "[132]\tvalid_0's multi_logloss: 1.63792\n",
      "[133]\tvalid_0's multi_logloss: 1.6366\n",
      "[134]\tvalid_0's multi_logloss: 1.6372\n",
      "[135]\tvalid_0's multi_logloss: 1.63693\n",
      "[136]\tvalid_0's multi_logloss: 1.63706\n",
      "[137]\tvalid_0's multi_logloss: 1.63649\n",
      "[138]\tvalid_0's multi_logloss: 1.63594\n",
      "[139]\tvalid_0's multi_logloss: 1.63426\n",
      "[140]\tvalid_0's multi_logloss: 1.63367\n",
      "[141]\tvalid_0's multi_logloss: 1.63306\n",
      "[142]\tvalid_0's multi_logloss: 1.63269\n",
      "[143]\tvalid_0's multi_logloss: 1.63232\n",
      "[144]\tvalid_0's multi_logloss: 1.63192\n",
      "[145]\tvalid_0's multi_logloss: 1.63338\n",
      "[146]\tvalid_0's multi_logloss: 1.63303\n",
      "[147]\tvalid_0's multi_logloss: 1.63234\n",
      "[148]\tvalid_0's multi_logloss: 1.63165\n",
      "[149]\tvalid_0's multi_logloss: 1.63005\n",
      "[150]\tvalid_0's multi_logloss: 1.6295\n",
      "[151]\tvalid_0's multi_logloss: 1.629\n",
      "[152]\tvalid_0's multi_logloss: 1.62802\n",
      "[153]\tvalid_0's multi_logloss: 1.62714\n",
      "[154]\tvalid_0's multi_logloss: 1.62746\n",
      "[155]\tvalid_0's multi_logloss: 1.62665\n",
      "[156]\tvalid_0's multi_logloss: 1.62575\n",
      "[157]\tvalid_0's multi_logloss: 1.6256\n",
      "[158]\tvalid_0's multi_logloss: 1.62488\n",
      "[159]\tvalid_0's multi_logloss: 1.62407\n",
      "[160]\tvalid_0's multi_logloss: 1.62324\n",
      "[161]\tvalid_0's multi_logloss: 1.62251\n",
      "[162]\tvalid_0's multi_logloss: 1.62309\n",
      "[163]\tvalid_0's multi_logloss: 1.62256\n",
      "[164]\tvalid_0's multi_logloss: 1.62251\n",
      "[165]\tvalid_0's multi_logloss: 1.62324\n",
      "[166]\tvalid_0's multi_logloss: 1.6222\n",
      "[167]\tvalid_0's multi_logloss: 1.62313\n",
      "[168]\tvalid_0's multi_logloss: 1.62278\n",
      "[169]\tvalid_0's multi_logloss: 1.62165\n",
      "[170]\tvalid_0's multi_logloss: 1.62098\n",
      "[171]\tvalid_0's multi_logloss: 1.62114\n",
      "[172]\tvalid_0's multi_logloss: 1.61936\n",
      "[173]\tvalid_0's multi_logloss: 1.61969\n",
      "[174]\tvalid_0's multi_logloss: 1.61996\n",
      "[175]\tvalid_0's multi_logloss: 1.6193\n",
      "[176]\tvalid_0's multi_logloss: 1.61926\n",
      "[177]\tvalid_0's multi_logloss: 1.61852\n",
      "[178]\tvalid_0's multi_logloss: 1.61779\n",
      "[179]\tvalid_0's multi_logloss: 1.61718\n",
      "[180]\tvalid_0's multi_logloss: 1.61795\n",
      "[181]\tvalid_0's multi_logloss: 1.61682\n",
      "[182]\tvalid_0's multi_logloss: 1.61665\n",
      "[183]\tvalid_0's multi_logloss: 1.617\n",
      "[184]\tvalid_0's multi_logloss: 1.61626\n",
      "[185]\tvalid_0's multi_logloss: 1.61563\n",
      "[186]\tvalid_0's multi_logloss: 1.61515\n",
      "[187]\tvalid_0's multi_logloss: 1.61528\n",
      "[188]\tvalid_0's multi_logloss: 1.6142\n",
      "[189]\tvalid_0's multi_logloss: 1.61353\n",
      "[190]\tvalid_0's multi_logloss: 1.61271\n",
      "[191]\tvalid_0's multi_logloss: 1.61214\n",
      "[192]\tvalid_0's multi_logloss: 1.61163\n",
      "[193]\tvalid_0's multi_logloss: 1.61101\n",
      "[194]\tvalid_0's multi_logloss: 1.61035\n",
      "[195]\tvalid_0's multi_logloss: 1.60999\n",
      "[196]\tvalid_0's multi_logloss: 1.60927\n",
      "[197]\tvalid_0's multi_logloss: 1.60923\n",
      "[198]\tvalid_0's multi_logloss: 1.60945\n",
      "[199]\tvalid_0's multi_logloss: 1.61013\n",
      "[200]\tvalid_0's multi_logloss: 1.60963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=1, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=255, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=80, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=200, nthread=-1, num_leaves=10,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as gbm\n",
    "import pandas as pd\n",
    "clf = gbm.sklearn.LGBMClassifier(n_estimators=200, num_leaves=10, min_child_samples=80)\n",
    "clf.fit(train_X, train_t, eval_set=(test_X, test_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "misc.imshow(misc.toimage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9858561540776407 0.44173441734417346\n"
     ]
    }
   ],
   "source": [
    "train_result = clf.predict(train_X)\n",
    "test_result = clf.predict(test_X)\n",
    "train_acc = len(train_t[train_result == train_t]) / len(train_t)\n",
    "test_acc = len(test_t[test_result == test_t]) / len(test_t)\n",
    "print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### result_df = pd.DataFrame(columns=['type_of_AE', 'input_size', 'reserved_all', 'augmentation', 'trees', 'train_acc', 'val_acc'])\n",
    "result_df.append(pd.Series({'type_of_AE':'conv', 'input_size':96, 'reserved_all':True, 'augmentation':False, 'trees': 200, 'train_acc':97.5, 'val_acc':37.8}), ignore_index=True)\n",
    "result_df.append(pd.Series({'type_of_AE':'conv', 'input_size':64, 'reserved_all':False, 'augmentation':False, 'trees': 200, 'train_acc':96.9, 'val_acc':36.9}), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc as misc\n",
    "import numpy as np\n",
    "path = '2/all/'\n",
    "imgs = []\n",
    "for i in range(640):\n",
    "    #print(i)\n",
    "    imgs.append(misc.imresize(misc.imread(path + str(i) + '.jpg', mode='L'), (128,128)).flatten()/255)\n",
    "df = pd.DataFrame(imgs)\n",
    "training = df.values.astype(np.uint8)\n",
    "np.save('testing_128', training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'arr' does not have a suitable array shape for any mode.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-193-281a6c9b91de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2/all/0.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/scipy/misc/pilutil.py\u001b[0m in \u001b[0;36mtoimage\u001b[0;34m(arr, high, low, cmin, cmax, pal, mode, channel_axis)\u001b[0m\n\u001b[1;32m    287\u001b[0m                                 ((3 in shape) or (4 in shape)))\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         raise ValueError(\"'arr' does not have a suitable array shape for \"\n\u001b[0m\u001b[1;32m    290\u001b[0m                          \"any mode.\")\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'arr' does not have a suitable array shape for any mode."
     ]
    }
   ],
   "source": [
    "misc.toimage(misc.imread('2/all/0.jpg', mode='L'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'arr' does not have a suitable array shape for any mode.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-af73bfc5e170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2/all/1.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/scipy/misc/pilutil.py\u001b[0m in \u001b[0;36mtoimage\u001b[0;34m(arr, high, low, cmin, cmax, pal, mode, channel_axis)\u001b[0m\n\u001b[1;32m    287\u001b[0m                                 ((3 in shape) or (4 in shape)))\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         raise ValueError(\"'arr' does not have a suitable array shape for \"\n\u001b[0m\u001b[1;32m    290\u001b[0m                          \"any mode.\")\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'arr' does not have a suitable array shape for any mode."
     ]
    }
   ],
   "source": [
    "misc.toimage('2/all/1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 16384)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 16384)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_size=128\n",
    "train_X = encoder.predict(df.values.reshape((len(df), img_size, img_size, 1)))\n",
    "dim = train_X.shape[1] * train_X.shape[2] * train_X.shape[3]\n",
    "train_X = train_X.reshape((len(train_X), dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07967082,  0.01951248,  0.05181229, ...,  0.        ,\n",
       "         0.        ,  0.2669189 ],\n",
       "       [ 0.04936669,  0.02024887,  0.13112801, ...,  0.        ,\n",
       "         0.        ,  0.24630128],\n",
       "       [ 0.16840397,  0.18514843,  0.0512196 , ...,  0.        ,\n",
       "         0.        ,  0.21914268],\n",
       "       ..., \n",
       "       [ 0.07407949,  0.010263  ,  0.03038684, ...,  0.        ,\n",
       "         0.        ,  0.1160766 ],\n",
       "       [ 0.10726933,  0.00459943,  0.06203297, ...,  0.        ,\n",
       "         0.        ,  0.33505374],\n",
       "       [ 0.10355181,  0.02542033,  0.03138468, ...,  0.        ,\n",
       "         0.        ,  0.22052495]], dtype=float32)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result = clf.predict(train_X)\n",
    "train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
